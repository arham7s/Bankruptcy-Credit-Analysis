# -*- coding: utf-8 -*-
"""Bankruptcy-Credit-Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UErbCHHUvzxSJmqX3-EeUEwg17b5_BRj

# ***Goal:***
The Goal of the project is to predict whether a company will go bankrupt in the following year, based on financial attributes of the company. Perhaps you are contemplating lending money to a company, and need to know whether the company is in near-term danger of not being able to repay.
"""

# Commented out IPython magic to ensure Python compatibility.
## Standard imports
import numpy as np
import pandas as pd

import sklearn

import os
import math

# %matplotlib inline

import sys
sys.path.append(r"/content/bankruptcy_helper.py")  # Add folder to Python path

  # Now try importing

"""# ***Helper API***"""

# Commented out IPython magic to ensure Python compatibility.
## Load the bankruptcy_helper module


InteractiveShell.ast_node_interactivity = "all"

# Reload all modules imported with %aimport
# %load_ext autoreload
# %autoreload 1

# Import bankruptcy_helper module
import bankruptcy_helper
# %aimport bankruptcy_helper

helper = bankruptcy_helper.Helper()

"""# ***Get the data***
The first step in our Recipe is Get the Data.

Each example is a row of data corresponding to a single company

*   There are 64 attributes, described in the section below
*   The column Bankrupt is 1 if the company subsequently went bankrupt; 0 if it did not go bankrupt
*   The column Id is a Company Identifier







"""

import pandas as pd

data_file = "5th_yr.csv"
data = pd.read_csv(data_file)

target_attr = "Bankrupt"
n_samples, n_attrs = data.shape
print("Data shape:", data.shape)

data.head()

data.info()

data.head()

"""data = data.drop(columns=["Id"], axis = 1)
data.head()
"""

data.shape

data = data.apply(pd.to_numeric, errors='coerce')
data = data.interpolate()
data.isnull().sum()

data.pivot_table(index='Bankrupt', aggfunc='size').plot(kind='bar', title = 'Class distribution')

data['Bankrupt'].value_counts()

from sklearn.model_selection import train_test_split

X = data.iloc[:,0:-1]
y = data.iloc[:, -1]
X,y = sklearn.utils.shuffle(X, y, random_state=42)
X_train = None
X_test = None
y_train = None
y_test = None

X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,test_size=0.2)

print("X_train shape: ", X_train.shape)
print("X_test shape: ", X_test.shape)
print("y_train shape: ", y_train.shape)
print("y_test shape: ", y_test.shape)

from imblearn.over_sampling import BorderlineSMOTE
# Resampling the minority class.
X_train,y_train = BorderlineSMOTE(random_state=10, k_neighbors=5).fit_resample(X_train,y_train)
X_train.shape

#Shuffling the dataset
X_train,y_train = sklearn.utils.shuffle(X_train, y_train, random_state=42)

from sklearn.model_selection import GridSearchCV
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
import pandas as pd
import os
from sklearn.model_selection import cross_val_score
from sklearn.metrics import RocCurveDisplay

from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, classification_report, precision_recall_curve, roc_curve, auc

# Replace any occurrences of "?" with NaN, which pandas can handle in correlation calculations
data = data.replace('?', np.nan)

# Convert all columns to numeric, coercing errors to NaN
data = data.apply(pd.to_numeric, errors='coerce')

# Interpolate missing values (NaN) if needed to fill gaps after replacing "?"
data = data.interpolate()

# Now you should be able to calculate the correlation matrix
data.corr()

# Replace any occurrences of "?" with NaN in X_train, which pandas can handle in correlation calculations
X_train = X_train.replace('?', np.nan)

# Convert all columns of X_train to numeric, coercing errors to NaN
X_train = X_train.apply(pd.to_numeric, errors='coerce')

# Interpolate missing values (NaN) if needed to fill gaps after replacing "?" in X_train
X_train = X_train.interpolate()

# Now you should be able to calculate the correlation matrix
corrmat = X_train.corr()
plt.subplots(figsize=(100,100))
hm = sns.heatmap(corrmat,
                 cbar=True,
                 annot=True,
                 square=True,
                 fmt='.2f',
                 annot_kws={'size': 25},
                 yticklabels=X_train.columns,
                 xticklabels=X_train.columns,
                 cmap="Spectral_r")
plt.show()

"""# **PCA**"""

scalar = StandardScaler()
X_train_scaled = scalar.fit_transform(X_train)

pca = PCA()
#
# Determine transformed features
#
X_train_pca = pca.fit_transform(X_train_scaled)
#
# Determine explained variance using explained_variance_ration_ attribute
#
exp_var_pca = pca.explained_variance_ratio_
#
# Cumulative sum of eigenvalues; This will be used to create step plot
# for visualizing the variance explained by each principal component.
#
cum_sum_eigenvalues = np.cumsum(exp_var_pca)
#
# Create the visualization plot
#
plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')
plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.tight_layout()
plt.show()

variance_goal_pct = 95
features_for_goal = helper.num_components_for_cum_variance(pca, .01 * variance_goal_pct)
print("To capture {f:d}% of variance we need {d:d} synthetic features.".format(f=variance_goal_pct, d=features_for_goal))

rf = RandomForestRegressor(n_estimators=150)
rf.fit(X_train_scaled, y_train)

sort = rf.feature_importances_.argsort()
plt.figure(figsize=(20, 20))
plt.barh(X_train.columns[sort], rf.feature_importances_[sort])
plt.xlabel("Feature Importance")

"""It can be seen that 25 synthetically created variables from PCA can explain 95% of the variance of target variable. This can be used as a pre-processing result for classifiers to reduce its overhead and to handle high-dimensionality problems.

This exercise creates a rough idea that X35(Profit On Sales/total assets) contributes maximum towards the variance of the target variable. Followed by X55(Working Capital), X22(Profit on Operating activities), X26(net profit + depreciation/ total liabilities) and so on. Overall the idea that for the companies to be strong and away from bankruptcy, profits are important is confirmed by PCA Analysis.

As the data is company Bankruptcy data, the basic assumption is that features in the dataset is linearly correlated and hence I am using feature engineering methods that handle linear data.

# ***Classifier Implementation***
## Recall vs Precision vs Accuracy
Recall: Predicting True Positives (TP) out of True Positives and False Negatives, i.e how accurately our model identified bankrupt companies out of the companies that are actually going to go bankrupt. Precision: Predicting True Positives (TP) out of True Positives and False Positive,i.e accuracy of correctly identifying companies that are bankrupt out of all the companies that are identified as bankrupt.

As specified in the instructions: "You may assume that it is 5 times worse to fail to identify a company that will go bankrupt than it is to fail to identify a company that won't go bankrupt.", recall becomes an important metric over precision. As we already know that the data given to us is highly imbalanced, taking accuracy as the performance metric of the classifier gives misleading results as eventhough I identfy all companies as not going bankrupt in a dataset that has 95% Class label '0', my model would be 95% accurate and would perform poorly on an unseen/out-of-sample data. For the classfiers that have high accuracy score might be because of correctly predicting True Negatives too, which for the moment is not a concern for this project. At the same time we want our model to spot as many real 1(Bankrupt) as possible over predicting 1(Bankrupt) as correct as possible, hence we choose Recall over Precision.

# ***LOGISTIC REGRESSION***
"""

def Logis_feature_selection(X_train,y_train):
    name = "Logistic Regression"
    scalar = StandardScaler()
    X_train_scaled = scalar.fit_transform(X_train)
    clf = LogisticRegression(solver = 'lbfgs',C=2.0,max_iter=10000)
    selector = SelectFromModel(clf)
    features_important = selector.fit_transform(X_train_scaled, y_train)
    clf.fit(features_important, y_train)
    return features_important

def Logis_CV(X_train,y_train,clf):
    print(f'cross_val_score (K = 5)')
    print(f'{cross_val_score(clf,X_train, y_train, cv = 5)}')

def Logis_Precision_recall_Curve_train(X_train,y_train,clf):
    y_train_pred_prob = clf.predict_proba(X_train)
    precision, recall, _ = precision_recall_curve(y_train, y_train_pred_prob[:,1])
    precision_recall = PrecisionRecallDisplay(precision = precision,recall = recall)
    precision_recall.plot()

def Logis_ROC_Curve_train(X_train,y_train,clf):
    plot_roc_curve(clf, X_train, y_train, name='Bankruptcy')

def Logis_Model(X_train,y_train,X):
    scalar = StandardScaler()
    X_train_scaled = scalar.fit_transform(X_train)
    clf = LogisticRegression(solver = 'lbfgs',C=2.0,max_iter=10000)
    selector = SelectFromModel(clf)
    features_important = selector.fit_transform(X_train_scaled, y_train)
    clf.fit(features_important, y_train)
    X_filtered = X.iloc[:,selector.get_support()]
    return clf,X_filtered

"""# ***Observations on Logistic Regression:***


*   For solver= 'lbfgs' and C=2.0 hyperparameters, I got the best result of Logistic Regression. I have chosen lbfgs over other solvers because it was giving me better convergence.
*  I observed that applying synthetically created features using PCA did not give me impressive results, I used SelectfromModel method which choose 17 features to input into the classifier and gave the best results.
*   Cross-Validation score, Training score and testing score were very close indicating a good fit model.
*   Area under the Curve (AUC) is 0.91 which is close to 1 indicating a good fit model. The Precision-Recall curve indicates the trade of between Precision and Recall.
*   I achieved highest testset recall of 78.5% for Logistic regression. But the accuracy is 82.8%. I have selected to print the results of logistic regression classifier with feature engineering over other models like Random Forest and KNN with higher accuracy scores because I am concerned about getting a model with high recall rate over accuracy and precision.

# ***Random Forest Classifier***
"""

def Rf_Model(X_train,y_train):
    pipeline = Pipeline(steps=[('Scaler',StandardScaler()),('PCA',PCA(n_components=17)),('rdf', RandomForestClassifier(n_estimators = 25,max_depth = 5,random_state=42))])
    pipeline.fit(X_train,y_train)
    return pipeline

def Rf_Model_with_feature_selection(X_train,y_train,X):
    scalar = StandardScaler()
    X_train_scaled = scalar.fit_transform(X_train)
    clf = RandomForestClassifier(n_estimators = 35,max_depth = 5,random_state=42)
    selector = SelectFromModel(clf)
    features_important = selector.fit_transform(X_train_scaled, y_train)
    clf.fit(features_important, y_train)
    X_filtered = X.iloc[:,selector.get_support()]
    return clf,X_filtered

def Rf_feature_selection(X_train,y_train):
    scalar = StandardScaler()
    X_train_scaled = scalar.fit_transform(X_train)
    clf = RandomForestClassifier(n_estimators = 35,max_depth = 5,random_state=42)
    selector = SelectFromModel(clf)
    features_important = selector.fit_transform(X_train_scaled, y_train)
    clf.fit(features_important, y_train)
    return features_important

def Rf_CV(X_train,y_train,clf):
    print(f'cross_val_score (K = 5)')
    print(f'{cross_val_score(clf,X_train, y_train, cv = 5)}')

def Rf_Precision_recall_Curve_train(X_train,y_train,clf):
    y_train_pred_prob = clf.predict_proba(X_train)
    precision, recall, _ = precision_recall_curve(y_train, y_train_pred_prob[:,1])
    precision_recall = PrecisionRecallDisplay(precision = precision,recall = recall)
    precision_recall.plot()

def Rf_ROC_Curve_train(X_train,y_train,clf):
    plot_roc_curve(clf, X_train, y_train, name='Bankruptcy')

"""# ***Observations on Random Forest Classifier***


1.   I tried Random Forest Classifier with feature engineering techniques like PCA and SelectFromModel and got higher accuracy but got inferior recall scores than Logistic regression with feature engineering for various values of hyperparamaeters of Random Forest.
2.  Eventhough RandomForest is a bagging Classifier which is a better classifier than Logistic regression on many cases due to its ensemble technique, for this unbalanced Bankruptcy dataset, it performed inferior to Logistic Regression on recall score parameter.
3. Area under the curve(AUC) is 0.97 indicates a got fit model. The Precision-Recall curve indicates the trade of between Precision and Recall.
4. With n_components = 25 and max_depth = 5, I got the best testset performance of Accuracy 84.3%% (Higher than Logistic Regression) but 61.5% Recall scores (Lower than Logistic Regression). Hence, I have chosen not to print the results of this model.
5. With the same hyperparameters and using SelectFromModel I got 88.6% accuracy and 67.7% recall.

# *KNN*
"""

def KNN_Feature_Selection(X_train,X_test,y_train,y_test,X):
    scalar = StandardScaler()
    X_train_scaled = scalar.fit_transform(X_train)
    X_test_scaled = scalar.transform(X_test)
    k_range = range(1,26)
    accuracy_scores = {}
    recall_scores = {}
    accuracy_scores_list = []
    recall_scores_list = []
    for k in k_range:
        knn = KNeighborsClassifier(n_neighbors = k)
        knn.fit(X_train_scaled,y_train)
        y_pred = knn.predict(X_test_scaled)
        accuracy_scores[k] = metrics.accuracy_score(y_test,y_pred)
        accuracy_scores_list.append(metrics.accuracy_score(y_test,y_pred))
        recall_scores[k] = metrics.recall_score(y_test,y_pred)
        recall_scores_list.append(metrics.recall_score(y_test,y_pred))

    plt.plot(k_range,accuracy_scores_list,'r')
    plt.plot(k_range,recall_scores_list,'b')
    plt.xlabel('Value of K for KNN')
    plt.ylabel('Testing Recall')

def KNN_Model(X_train,y_train):
    scalar = StandardScaler()
    X_train_scaled = scalar.fit_transform(X_train)
    knn = KNeighborsClassifier(n_neighbors = 18)
    knn.fit(X_train_scaled,y_train)
    pipeline = Pipeline(steps=[('Scaler',StandardScaler()),('PCA',PCA(n_components=25)),('knn', KNeighborsClassifier(n_neighbors=18))])
    pipeline.fit(X_train,y_train)
    #return knn
    return pipeline

def KNN_Precision_recall_Curve_train(X_train,y_train,clf):
    y_train_pred_prob = clf.predict_proba(X_train)
    precision, recall, _ = precision_recall_curve(y_train, y_train_pred_prob[:,1])
    precision_recall = PrecisionRecallDisplay(precision = precision,recall = recall)
    precision_recall.plot()

def KNN_ROC_Curve_train(X_train,y_train,clf):
    plot_roc_curve(clf, X_train, y_train, name='Bankruptcy')

"""# ***Observations on KNN Classifier***
1. I displayed the graph of precision vs recall with changes in K values to find the optimium K value which is 18.
2. Using feature engineering technique of PCA improved the results of the model and gave an accuracy score of 82.4% and recall score of 67.7% on the test dataset.
3. Area under the curve(AUC) is 0.97 indicates a got fit model. The Precision-Recall curve indicates the trade of between Precision and Recall.
4. I have not chosen this model because it has inferior recall score compared to Logistic Regression eventhough it has higher accuracy scores.

# ***SVM***
"""

def SVM_Model(X_train,y_train):
    pipeline = Pipeline(steps=[('Scaler',StandardScaler()),('pca',PCA(n_components=25)),('svm', SVC(kernel='sigmoid',C=1.3,random_state=42,probability=True))])
    pipeline.fit(X_train,y_train)
    return pipeline

def SVM_Model_with_feature_selection(X_train,y_train,X):
    scalar = StandardScaler()
    X_train_scaled = scalar.fit_transform(X_train)
    clf = SVC(kernel='sigmoid',C=1.3,random_state=42,probability=True)
    selector = SelectFromModel(clf)
    features_important = selector.fit_transform(X_train_scaled, y_train)
    clf.fit(features_important, y_train)
    X_filtered = X.iloc[:,selector.get_support()]
    return clf,X_filtered
    clf = SVC(kernel='sigmoid',C=1.3,random_state=42,probability=True)
    selector = SelectFromModel(clf)
    features_important = selector.fit_transform(X_train_scaled, y_train)
    clf.fit(features_important, y_train)
    X_filtered = X.iloc[:,selector.get_support()]
    return clf,X_filtered

def SVM_feature_selection(X_train,y_train):
    scalar = StandardScaler()
    X_train_scaled = scalar.fit_transform(X_train)
    clf = SVC(kernel='sigmoid',C=1.3,random_state=42,probability=True)
    selector = SelectFromModel(clf)
    features_important = selector.fit_transform(X_train_scaled, y_train)
    clf.fit(features_important, y_train)
    return features_important

def SVM_CV(X_train,y_train,clf):
    print(f'{cross_val_score(clf,X_train, y_train, cv = 5)}')

def SVM_Precision_recall_Curve_train(X_train,y_train,clf):
    y_train_pred_prob = clf.predict_proba(X_train)
    precision, recall, _ = precision_recall_curve(y_train, y_train_pred_prob[:,1])
    precision_recall = PrecisionRecallDisplay(precision = precision,recall = recall)
    precision_recall.plot()

def SVM_ROC_Curve_train(X_train,y_train,clf):
    plot_roc_curve(clf, X_train, y_train, name='Bankruptcy')

"""# ***Observations on SVM Classifier***
1. I used sigmoid kernel which is better suited for binary classification and adjusted hyperparameter C= 1.3 to get the max recall score from the model.
2. I applied PCA on 25 components as a feature engineering technique to increase the recall score of the model.
3. I observed an accuracy score of 70.9% and recall score of 75.4% on test data set which is better than KNN and Random Forest in terms of recall score but is lesser than the logistic regression model.

# ***Check your work: predict and evaluate metrics on your test examples***
1. Although only the instructors have the correct labels for the holdout dataset, you may want to create your own test dataset on which to evaluate your out of sample metrics.

2. If you choose to do so, you can evaluate your models using the same metrics that the instructors will use.

3. Test whether your implementation of MyModel works. See the metrics your model produces.
"""

def logis_testdata_prepare(X_train,y_train,X):
    X = X.apply(pd.to_numeric, errors='coerce')
    X = X.interpolate()
    scalar = StandardScaler()
    X_train_scaled = scalar.fit_transform(X_train)
    X = pd.DataFrame(scalar.transform(X))
    return X

def testdata_prepare(X_train,y_train,X):
    X = X.apply(pd.to_numeric, errors='coerce')
    X = X.interpolate()
    return X

name = 'Logistic Regression Classifier'

import pandas as pd
import os
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, classification_report, precision_recall_curve


def MyModel(X):
    # It should create an array of predictions; we initialize it to the empty array for convenience
    # YOUR CODE GOES HERE
    predictions = []
    clf = None


    if name == 'Logistic Regression Classifier':
        X = logis_testdata_prepare(X_train,y_train,X)
        features_important = Logis_feature_selection(X_train,y_train)
        clf, X_filtered = Logis_Model(X_train,y_train,X)
        #Logis_CV(X_train,y_train,clf)
        Logis_Precision_recall_Curve_train(features_important,y_train,clf)
        Logis_ROC_Curve_train(features_important,y_train,clf)
        predictions = clf.predict(X_filtered)
    elif name == 'Random Forest Classifier':
        X = testdata_prepare(X_train,y_train,X)
        clf = Rf_Model(X_train,y_train)
        Rf_CV(X_train,y_train,clf)
        Rf_Precision_recall_Curve_train(X_train,y_train,clf)
        Rf_ROC_Curve_train(X_train,y_train,clf)
        predictions = clf.predict(X)
    elif name == 'KNN Classifier':
        X = testdata_prepare(X_train,y_train,X)
        KNN_Feature_Selection(X_train,X_test,y_train,y_test,X)
        clf = KNN_Model(X_train,y_train)
        KNN_Precision_recall_Curve_train(X_train,y_train,clf)
        KNN_ROC_Curve_train(X_train,y_train,clf)
        predictions = clf.predict(X)
    elif name == 'Random Forest Classifier with Feature Selection':
        X = logis_testdata_prepare(X_train,y_train,X)
        features_important = Rf_feature_selection(X_train,y_train)
        clf, X_filtered = Rf_Model_with_feature_selection(X_train,y_train,X)
        Rf_CV(X_train,y_train,clf)
        Rf_Precision_recall_Curve_train(features_important,y_train,clf)
        Rf_ROC_Curve_train(features_important,y_train,clf)
        predictions = clf.predict(X_filtered)
    elif name == 'Support Vector Machine Classifier':
        X = testdata_prepare(X_train,y_train,X)
        clf = SVM_Model(X_train,y_train)
        SVM_CV(X_train,y_train,clf)
        SVM_Precision_recall_Curve_train(X_train,y_train,clf)
        SVM_ROC_Curve_train(X_train,y_train,clf)
        predictions = clf.predict(X)
    return predictions

def Metrics(X_test,y_test):

    #print("Training Metrics: ")
    #y_test_pred = MyModel(X_train)
    #F1_score_train = f1_score(y_train, y_test_pred)
    #accuracy_train = accuracy_score(y_train, y_test_pred)
    #recall_train = recall_score(y_train, y_test_pred, pos_label=1, average="binary")
    #precision_train = precision_score(y_train,   y_test_pred, pos_label=1, average="binary")

    #print("\t{m:s} Accuracy: {a:3.1%}, Recall {r:3.1%}, Precision {p:3.1%}, F-1 score {f:3.1%}".format(m=name,
    #                                                                            a=accuracy_train,
    #                                                                            r=recall_train,
    #                                                                            p=precision_train,
    #                                                                            f = F1_score_train
    #                                                                            )
    #         )
    #confusion_matrix(y_train,y_test_pred)
    #pd.crosstab(y_train, y_test_pred, rownames = ['Actual'], colnames =['Predicted'], margins = True)
    #print(classification_report(y_train, y_test_pred))

    print("Testing Metrics: ")
    y_test_pred = MyModel(X_test)
    accuracy_test = accuracy_score(y_test, y_test_pred)
    F1_score_test = f1_score(y_test, y_test_pred)
    recall_test = recall_score(y_test, y_test_pred, pos_label=1, average="binary")
    precision_test = precision_score(y_test,   y_test_pred, pos_label=1, average="binary")

    print("\t{m:s} Accuracy: {a:3.1%}, Recall {r:3.1%}, Precision {p:3.1%}, F-1 score {f:3.1%}".format(m=name,
                                                                                a=accuracy_test,
                                                                                r=recall_test,
                                                                                p=precision_test,
                                                                                f = F1_score_test
                                                                                )
             )
    confusion_matrix(y_test,y_test_pred)
    pd.crosstab(y_test, y_test_pred, rownames = ['Actual'], colnames =['Predicted'], margins = True)
    print(classification_report(y_test, y_test_pred))

from sklearn.metrics import RocCurveDisplay # Import RocCurveDisplay

def Logis_ROC_Curve_train(X_train,y_train,clf):
    RocCurveDisplay.from_estimator(clf, X_train, y_train, name='Bankruptcy') # Use RocCurveDisplay.from_estimator
    plt.show() # Add plt.show() to display the plot

print(f"The name of the Classifier is: {name}")
#X_test, y_test = X_hold, y_hold   #Remove the comment at the start of this line before running this model
Metrics(X_test,y_test)

"""# ***RESULT***"""

result = {
        'Training Accuracy': [85.5,88,90.9,90.3,74.2],
        'Training Precision': [85.0,87.1,90.7,86.8,78],
        'Training Recall': [86.2,89.2,91.2,95.2,91.5],
        'Testing Accuracy': [82.8,84.3,88.6,82.4,70.9],
        'Testing Precision': [25.1,24.1,33.1,22.8,15.6],
        'Testing Recall': [78.5,61.5,67.7,67.7,75.4],
        }

df = pd.DataFrame(data=result, index = ['Logistic Regression', 'Random Forest with PCA', 'Random Forest with SelectFromModel','KNN with PCA','SVM with PCA'])

print(df)

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, classification_report

def train_logistic_model(X, y, test_size=0.2, random_state=42):
    """
    Train a Logistic Regression model to predict bankruptcy probability.

    Parameters:
    - X: ndarray or DataFrame of financial features
    - y: binary labels (0 = Healthy, 1 = Bankrupt)

    Returns:
    - model: trained logistic regression model
    - X_test, y_test: test set for evaluation
    """
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    model = LogisticRegression(max_iter=1000)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]

    print("Model Evaluation:")
    print(classification_report(y_test, y_pred))
    print(f"AUC Score: {roc_auc_score(y_test, y_prob):.4f}")

    return model, X_test, y_test

def predict_bankruptcy_prob(model, X, firm_names=None):
    """
    Predict bankruptcy probabilities using the trained model.

    Parameters:
    - model: trained logistic regression model
    - X: input data
    - firm_names: optional list of firm names

    Returns:
    - DataFrame with firm names and bankruptcy probabilities
    """
    prob = model.predict_proba(X)[:, 1]
    df = pd.DataFrame({
        "Firm": firm_names if firm_names is not None else range(len(prob)),
        "Bankruptcy_Probability (%)": (prob * 100).round(2)
    })
    return df.sort_values(by="Bankruptcy_Probability (%)", ascending=False)

def compute_altman_z_score(X1, X2, X3, X4, X5):
    """
    Compute the Altman Z-score.

    Z = 1.2*X1 + 1.4*X2 + 3.3*X3 + 0.6*X4 + 1.0*X5

    Parameters:
    - X1 to X5: financial ratios as Series or arrays

    Returns:
    - z_scores: ndarray of Z-score values
    """
    Z = 1.2 * X1 + 1.4 * X2 + 3.3 * X3 + 0.6 * X4 + 1.0 * X5
    return Z

def classify_altman_z_score(z_scores):
    """
    Classify financial health based on Z-score thresholds.

    - Z > 2.99 → Safe Zone
    - 1.81 < Z <= 2.99 → Grey Zone
    - Z <= 1.81 → Distress Zone

    Parameters:
    - z_scores: array of Z-scores

    Returns:
    - list of categories
    """
    categories = []
    for z in z_scores:
        if z > 2.99:
            categories.append("Safe")
        elif z > 1.81:
            categories.append("Grey Zone")
        else:
            categories.append("Distress")
    return categories

from sklearn.ensemble import RandomForestClassifier

def train_credit_rating_model(X, y_ratings, test_size=0.2, random_state=42):
    """
    Train a credit rating classifier using financial features.

    Parameters:
    - X: financial features
    - y_ratings: categorical credit ratings (e.g., AAA, BBB, CCC, Default)

    Returns:
    - model: trained classifier
    - X_test, y_test: test data for evaluation
    """
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_ratings, test_size=test_size, random_state=random_state
    )
    clf = RandomForestClassifier(n_estimators=100, random_state=random_state)
    clf.fit(X_train, y_train)

    print("Credit Rating Classification Report:")
    print(classification_report(y_test, clf.predict(X_test)))

    return clf, X_test, y_test

def predict_credit_rating(model, X, firm_names=None):
    """
    Predict credit ratings from financial data.

    Parameters:
    - model: trained classifier
    - X: financial data
    - firm_names: optional firm identifiers

    Returns:
    - DataFrame of predicted ratings
    """
    preds = model.predict(X)
    df = pd.DataFrame({
        "Firm": firm_names if firm_names is not None else range(len(preds)),
        "Predicted_Credit_Rating": preds
    })
    return df

import pandas as pd

df = pd.read_csv("5th_yr.csv")
df.head()

X = df.drop(columns=["Bankrupt", "Id"], axis=1)  # keep only features
y = df["Bankrupt"]
firm_names = df["Id"] if "Id" in df.columns else None

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, classification_report

def train_logistic_model(X, y, test_size=0.2, random_state=42):
    """
    Train a Logistic Regression model to predict bankruptcy probability.

    Parameters:
    - X: ndarray or DataFrame of financial features
    - y: binary labels (0 = Healthy, 1 = Bankrupt)

    Returns:
    - model: trained logistic regression model
    - X_test, y_test: test set for evaluation
    """

    # Convert all columns of X to numeric, coercing errors to NaN
    X = X.apply(pd.to_numeric, errors='coerce')

    # Interpolate missing values (NaN) if needed to fill gaps after replacing "?"
    X = X.interpolate()

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    model = LogisticRegression(max_iter=1000)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]

    print("Model Evaluation:")
    print(classification_report(y_test, y_pred))
    print(f"AUC Score: {roc_auc_score(y_test, y_prob):.4f}")

    return model, X_test, y_test

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, classification_report

def train_logistic_model(X, y, test_size=0.2, random_state=42):
    """
    Train a Logistic Regression model to predict bankruptcy probability.

    Parameters:
    - X: ndarray or DataFrame of financial features
    - y: binary labels (0 = Healthy, 1 = Bankrupt)

    Returns:
    - model: trained logistic regression model
    - X_test, y_test: test set for evaluation
    """

    # Convert all columns of X to numeric, coercing errors to NaN
    X = X.apply(pd.to_numeric, errors='coerce')

    # Interpolate missing values (NaN) if needed to fill gaps after replacing "?"
    X = X.interpolate()

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    model = LogisticRegression(max_iter=1000)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]

    print("Model Evaluation:")
    print(classification_report(y_test, y_pred))
    print(f"AUC Score: {roc_auc_score(y_test, y_prob):.4f}")

    return model, X_test, y_test

def predict_bankruptcy_prob(model, X, firm_names=None):
    """
    Predict bankruptcy probabilities using the trained model.

    Parameters:
    - model: trained logistic regression model
    - X: input data
    - firm_names: optional list of firm names

    Returns:
    - DataFrame with firm names and bankruptcy probabilities
    """
    # Preprocess the input data X in the same way as the training data
    X = X.apply(pd.to_numeric, errors='coerce')  # Convert to numeric
    X = X.interpolate()  # Interpolate missing values

    prob = model.predict_proba(X)[:, 1]
    df = pd.DataFrame({
        "Firm": firm_names if firm_names is not None else range(len(prob)),
        "Bankruptcy_Probability (%)": (prob * 100).round(2)
    })
    return df.sort_values(by="Bankruptcy_Probability (%)", ascending=False)

model, X_test, y_test = train_logistic_model(X, y)

prob_df = predict_bankruptcy_prob(model, X, firm_names)
display(prob_df.head(10))  # Show top 10 firms with highest bankruptcy risk

import pandas as pd

def compute_altman_z_score(X1, X2, X3, X4, X5):
    """
    Compute the Altman Z-score.

    Z = 1.2*X1 + 1.4*X2 + 3.3*X3 + 0.6*X4 + 1.0*X5

    Parameters:
    - X1 to X5: financial ratios as Series or arrays

    Returns:
    - z_scores: ndarray of Z-score values
    """
    # Convert X1 to X5 to numeric
    X1 = pd.to_numeric(X1, errors='coerce')
    X2 = pd.to_numeric(X2, errors='coerce')
    X3 = pd.to_numeric(X3, errors='coerce')
    X4 = pd.to_numeric(X4, errors='coerce')
    X5 = pd.to_numeric(X5, errors='coerce')

    Z = 1.2 * X1 + 1.4 * X2 + 3.3 * X3 + 0.6 * X4 + 1.0 * X5
    return Z

def convert_z_to_rating(z):
    if z > 3.0:
        return "AAA"
    elif z > 2.0:
        return "BBB"
    elif z > 1.0:
        return "CCC"
    else:
        return "Default"

# Assuming X1, X2, X3, X4, X5 are the relevant columns in your DataFrame 'df'
Z = compute_altman_z_score(df['X1'], df['X2'], df['X3'], df['X4'], df['X5'])

df['Rating'] = Z.map(convert_z_to_rating) # Now Z is defined and can be used

df['Altman_Rating'] = Z.map(convert_z_to_rating)

z_df = pd.DataFrame({
    "Firm": firm_names,
    "Altman Z-Score": Z.round(2),
    "Rating": df["Altman_Rating"]
})
display(z_df.head(10))

df['Altman_Rating'] = Z.map(convert_z_to_rating)

z_df = pd.DataFrame({
    "Firm": firm_names,
    "Altman Z-Score": Z.round(2),
    "Rating": df["Altman_Rating"]
})
display(z_df.tail(10))

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

def train_credit_rating_model(X, y_ratings, test_size=0.2, random_state=42):
    """
    Train a credit rating classifier using financial features.

    Parameters:
    - X: financial features
    - y_ratings: categorical credit ratings (e.g., AAA, BBB, CCC, Default)

    Returns:
    - model: trained classifier
    - X_test, y_test: test data for evaluation
    """

    # Replace any occurrences of "?" with NaN, which pandas can handle in correlation calculations
    X = X.replace('?', np.nan)

    # Convert all columns to numeric, coercing errors to NaN
    X = X.apply(pd.to_numeric, errors='coerce')

    # Interpolate missing values (NaN) if needed to fill gaps after replacing "?"
    X = X.interpolate()

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_ratings, test_size=test_size, random_state=random_state
    )
    clf = RandomForestClassifier(n_estimators=100, random_state=random_state)
    clf.fit(X_train, y_train)

    print("Credit Rating Classification Report:")
    print(classification_report(y_test, clf.predict(X_test)))

    return clf, X_test, y_test

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

def train_credit_rating_model(X, y_ratings, test_size=0.2, random_state=42):
    """
    Train a credit rating classifier using financial features.

    Parameters:
    - X: financial features
    - y_ratings: categorical credit ratings (e.g., AAA, BBB, CCC, Default)

    Returns:
    - model: trained classifier
    - X_test, y_test: test data for evaluation
    """

    # Replace any occurrences of "?" with NaN, which pandas can handle in correlation calculations
    X = X.replace('?', np.nan)

    # Convert all columns to numeric, coercing errors to NaN
    X = X.apply(pd.to_numeric, errors='coerce')

    # Interpolate missing values (NaN) if needed to fill gaps after replacing "?"
    X = X.interpolate()

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_ratings, test_size=test_size, random_state=random_state
    )
    clf = RandomForestClassifier(n_estimators=100, random_state=random_state)
    clf.fit(X_train, y_train)

    print("Credit Rating Classification Report:")
    print(classification_report(y_test, clf.predict(X_test)))

    return clf, X_test, y_test

def predict_credit_rating(model, X, firm_names=None):
    """
    Predict credit ratings from financial data.

    Parameters:
    - model: trained classifier
    - X: financial data
    - firm_names: optional firm identifiers

    Returns:
    - DataFrame of predicted ratings
    """
    # Preprocess X before prediction (similar to what's done in train_credit_rating_model)
    X = X.replace('?', np.nan)
    X = X.apply(pd.to_numeric, errors='coerce')
    X = X.interpolate()

    preds = model.predict(X)
    df = pd.DataFrame({
        "Firm": firm_names if firm_names is not None else range(len(preds)),
        "Predicted_Credit_Rating": preds
    })
    return df

rating_model, _, _ = train_credit_rating_model(X, df['Altman_Rating'])
rating_df = predict_credit_rating(rating_model, X, firm_names)
display(rating_df.head(10))


import streamlit as st
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score

# --------------------- Utility Functions ---------------------

def train_logistic_model(X, y):
    X = X.apply(pd.to_numeric, errors='coerce').interpolate()
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model = LogisticRegression(max_iter=1000)
    model.fit(X_train, y_train)
    return model, X

def predict_bankruptcy_prob(model, X, firm_names=None):
    X = X.apply(pd.to_numeric, errors='coerce').interpolate()
    prob = model.predict_proba(X)[:, 1]
    return pd.DataFrame({
        "Firm": firm_names if firm_names is not None else range(len(prob)),
        "Bankruptcy Probability (%)": (prob * 100).round(2)
    }).sort_values(by="Bankruptcy Probability (%)", ascending=False)

def compute_altman_z_score(X1, X2, X3, X4, X5):
    Z = 1.2 * pd.to_numeric(X1, errors='coerce') + \
        1.4 * pd.to_numeric(X2, errors='coerce') + \
        3.3 * pd.to_numeric(X3, errors='coerce') + \
        0.6 * pd.to_numeric(X4, errors='coerce') + \
        1.0 * pd.to_numeric(X5, errors='coerce')
    return Z

def convert_z_to_rating(z):
    if z > 3.0:
        return "AAA"
    elif z > 2.0:
        return "BBB"
    elif z > 1.0:
        return "CCC"
    else:
        return "Default"

def train_credit_rating_model(X, y_ratings):
    X = X.apply(pd.to_numeric, errors='coerce').interpolate()
    X_train, X_test, y_train, y_test = train_test_split(X, y_ratings, test_size=0.2, random_state=42)
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train, y_train)
    return clf

def predict_credit_rating(model, X, firm_names=None):
    X = X.apply(pd.to_numeric, errors='coerce').interpolate()
    preds = model.predict(X)
    return pd.DataFrame({
        "Firm": firm_names if firm_names is not None else range(len(preds)),
        "Predicted Credit Rating": preds
    })

# --------------------- Streamlit UI ---------------------

st.set_page_config(page_title="Bankruptcy Credit Analysis Dashboard", layout="wide")
st.title("💸 Bankruptcy Risk & Credit Rating Analysis")

uploaded_file = st.file_uploader("Upload your financial dataset (.csv)", type=["csv"])

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.success("File uploaded successfully!")
    st.dataframe(df.head())

    if 'Bankrupt' not in df.columns:
        st.error("Column 'Bankrupt' not found in dataset. Required for model training.")
    else:
        # Extract data
        X = df.drop(columns=['Bankrupt', 'Id'], errors='ignore')
        y = df['Bankrupt']
        firm_names = df['Id'] if 'Id' in df.columns else None

        # Bankruptcy Probability Model
        model, X_full = train_logistic_model(X, y)
        prob_df = predict_bankruptcy_prob(model, X_full, firm_names)

        # Altman Z-Score
        try:
            Z = compute_altman_z_score(df['X1'], df['X2'], df['X3'], df['X4'], df['X5'])
            ratings = Z.map(convert_z_to_rating)
            altman_df = pd.DataFrame({
                "Firm": firm_names if firm_names is not None else range(len(Z)),
                "Altman Z-Score": Z.round(2),
                "Altman Zone": ratings
            })
        except Exception as e:
            st.error(f"Error computing Altman Z-score: {e}")
            altman_df = None

        # Credit Rating Classifier
        if altman_df is not None:
            clf = train_credit_rating_model(X, ratings)
            rating_df = predict_credit_rating(clf, X, firm_names)
        else:
            rating_df = None

        # Merge Outputs
        final = prob_df.set_index("Firm")
        if altman_df is not None:
            final = final.join(altman_df.set_index("Firm"))
        if rating_df is not None:
            final = final.join(rating_df.set_index("Firm"))

        st.subheader("🔢 Risk Scores and Ratings")
        st.dataframe(final.reset_index().sort_values(by="Bankruptcy Probability (%)", ascending=False))
